{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e64e67d",
   "metadata": {},
   "source": [
    "1. Funções de ativação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a6f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Funções de ativação\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivada_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857adac",
   "metadata": {},
   "source": [
    "2. Classe Neuronio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446a6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Neurônio\n",
    "class Neuronio:\n",
    "    def __init__(self, num_entradas, funcao_ativacao, derivada_ativacao):\n",
    "        self.pesos = np.random.randn(num_entradas) * np.sqrt(2.0 / num_entradas)  # Inicialização Xavier\n",
    "        self.bias = np.random.randn() * 0.01\n",
    "        self.funcao_ativacao = funcao_ativacao\n",
    "        self.derivada_ativacao = derivada_ativacao\n",
    "        self.saida = None\n",
    "        self.delta = None\n",
    "\n",
    "    def calcular_saida(self, entradas):\n",
    "        self.entradas = entradas\n",
    "        soma_ponderada = np.dot(self.entradas, self.pesos) + self.bias\n",
    "        self.saida = self.funcao_ativacao(soma_ponderada)\n",
    "        return self.saida\n",
    "\n",
    "    def atualizar_pesos(self, taxa_aprendizado, entrada_anterior):\n",
    "        self.pesos += taxa_aprendizado * entrada_anterior * self.delta\n",
    "        self.bias += taxa_aprendizado * self.delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd7f06",
   "metadata": {},
   "source": [
    "3. Classe Camada:\n",
    "   Uma camada é um conjunto de neurônios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b59c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Camada\n",
    "class Camada:\n",
    "    def __init__(self, num_neuronios, num_entradas, funcao_ativacao, derivada_ativacao):\n",
    "        self.neuronios = [Neuronio(num_entradas, funcao_ativacao, derivada_ativacao) for _ in range(num_neuronios)]\n",
    "        self.saidas = None\n",
    "        self.entradas = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward(self, entradas):\n",
    "        self.entradas = entradas\n",
    "        self.saidas = np.array([neuronio.calcular_saida(entradas) for neuronio in self.neuronios])\n",
    "        return self.saidas\n",
    "\n",
    "    def backward(self, erro_camada_posterior, pesos_camada_posterior, derivada_ativacao_saidas):\n",
    "        delta_atual = np.dot(erro_camada_posterior, pesos_camada_posterior) * derivada_ativacao_saidas\n",
    "        self.delta = delta_atual\n",
    "        for i, neuronio in enumerate(self.neuronios):\n",
    "            neuronio.delta = delta_atual[i]\n",
    "        return delta_atual\n",
    "\n",
    "    def atualizar_pesos(self, taxa_aprendizado, entradas_camada_anterior):\n",
    "        for neuronio in self.neuronios:\n",
    "            neuronio.atualizar_pesos(taxa_aprendizado, entradas_camada_anterior)\n",
    "\n",
    "    def obter_pesos(self):\n",
    "        return np.array([neuronio.pesos for neuronio in self.neuronios])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84c811",
   "metadata": {},
   "source": [
    "4. Classe MLP:\n",
    "\n",
    "   A rede MLP é composta por várias camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f577a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe MLP\n",
    "class MLP:\n",
    "    def __init__(self, num_entradas, arquitetura, funcao_ativacao, derivada_ativacao):\n",
    "        self.camadas = []\n",
    "        num_neuronios_anterior = num_entradas\n",
    "        for num_neuronios in arquitetura:\n",
    "            self.camadas.append(Camada(num_neuronios, num_neuronios_anterior, funcao_ativacao, derivada_ativacao))\n",
    "            num_neuronios_anterior = num_neuronios\n",
    "\n",
    "    def forward(self, entrada):\n",
    "        saida_atual = entrada\n",
    "        for camada in self.camadas:\n",
    "            saida_atual = camada.forward(saida_atual)\n",
    "        return saida_atual\n",
    "\n",
    "    def backward(self, saida_esperada, saida_predita):\n",
    "        erro = saida_esperada - saida_predita\n",
    "        num_camadas = len(self.camadas)\n",
    "        for i in reversed(range(num_camadas)):\n",
    "            camada_atual = self.camadas[i]\n",
    "            if i == num_camadas - 1:  # Camada de saída\n",
    "                delta_atual = erro * derivada_sigmoid(camada_atual.saidas)\n",
    "                for j, neuronio in enumerate(camada_atual.neuronios):\n",
    "                    neuronio.delta = delta_atual[j]\n",
    "            else:  # Camadas ocultas\n",
    "                camada_posterior = self.camadas[i + 1]\n",
    "                pesos_camada_posterior = camada_posterior.obter_pesos()\n",
    "                delta_atual = camada_atual.backward(delta_atual, pesos_camada_posterior, derivada_sigmoid(camada_atual.saidas))\n",
    "                for j, neuronio in enumerate(camada_atual.neuronios):\n",
    "                    neuronio.delta = delta_atual[j]\n",
    "            camada_atual.delta = delta_atual\n",
    "\n",
    "    def atualizar_pesos(self, taxa_aprendizado, entrada):\n",
    "        entrada_anterior = entrada\n",
    "        for camada in self.camadas:\n",
    "            camada.atualizar_pesos(taxa_aprendizado, entrada_anterior)\n",
    "            entrada_anterior = camada.saidas\n",
    "\n",
    "    def treinar(self, dados_treinamento, rotulos_treinamento, taxa_aprendizado, num_epocas):\n",
    "        for epoca in range(num_epocas):\n",
    "            perda_total = 0\n",
    "            for i, entrada in enumerate(dados_treinamento):\n",
    "                saida_predita = self.forward(entrada)\n",
    "                saida_esperada = rotulos_treinamento[i]\n",
    "                perda = np.mean((saida_esperada - saida_predita) ** 2)\n",
    "                perda_total += perda\n",
    "                self.backward(saida_esperada, saida_predita)\n",
    "                self.atualizar_pesos(taxa_aprendizado, entrada)\n",
    "            print(f\"Época {epoca + 1}/{num_epocas}, Perda: {perda_total / len(dados_treinamento):.6f}\")\n",
    "\n",
    "    def predizer(self, dados_teste):\n",
    "        predicoes = []\n",
    "        for entrada in dados_teste:\n",
    "            saida = self.forward(entrada)\n",
    "            predicao = np.argmax(saida)\n",
    "            predicoes.append(predicao)\n",
    "        return predicoes\n",
    "\n",
    "    def obter_pesos_treinados(self):\n",
    "        pesos_treinados = []\n",
    "        for i, camada in enumerate(self.camadas):\n",
    "            pesos_camada = camada.obter_pesos()\n",
    "            bias_camada = np.array([neuronio.bias for neuronio in camada.neuronios])\n",
    "            pesos_treinados.append((f\"Camada {i+1}\", pesos_camada, bias_camada))\n",
    "        return pesos_treinados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aef94",
   "metadata": {},
   "source": [
    "5. Carregar os Dados e pré processar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3907005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de carregamento e pré-processamento\n",
    "def carregar_dados(nome_arquivo):\n",
    "    try:\n",
    "        df = pd.read_csv(nome_arquivo, header=None)\n",
    "        dados = df.iloc[:, :-1].values.astype(float)  # Características (A, B, C, D)\n",
    "        rotulos = df.iloc[:, -1].values  # Classes (E)\n",
    "        return dados, rotulos\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{nome_arquivo}' não encontrado.\")\n",
    "        return None, None\n",
    "\n",
    "def normalizar_dados(dados):\n",
    "    minimos = np.min(dados, axis=0)\n",
    "    maximos = np.max(dados, axis=0)\n",
    "    range_valores = maximos - minimos\n",
    "    range_valores[range_valores == 0] = 1\n",
    "    return (dados - minimos) / range_valores\n",
    "\n",
    "def codificar_rotulos(rotulos):\n",
    "    classes = np.unique(rotulos)\n",
    "    num_classes = len(classes)\n",
    "    mapeamento = {classe: i for i, classe in enumerate(classes)}\n",
    "    rotulos_codificados = np.zeros((len(rotulos), num_classes))\n",
    "    for i, rotulo in enumerate(rotulos):\n",
    "        indice_classe = mapeamento[rotulo]\n",
    "        rotulos_codificados[i, indice_classe] = 1\n",
    "    return rotulos_codificados, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51822708",
   "metadata": {},
   "source": [
    "6. Normalizar e Codificar Rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4448be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n",
      "Número de instâncias de treinamento: 120\n",
      "Número de instâncias de teste: 30\n",
      "Dados normalizados com sucesso!\n",
      "Primeiras 5 instâncias de treinamento normalizadas:\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]]\n",
      "Primeiras 5 instâncias de teste normalizadas:\n",
      "[[0.16       0.46666667 0.02173913 0.04347826]\n",
      " [0.28       1.         0.06521739 0.        ]\n",
      " [0.08       0.6        0.02173913 0.        ]\n",
      " [0.36       0.93333333 0.04347826 0.        ]\n",
      " [0.24       0.66666667 0.02173913 0.        ]]\n",
      "Rótulos codificados com sucesso!\n",
      "Mapeamento de classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "Primeiros 5 rótulos de treinamento codificados:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "Primeiros 5 rótulos de teste codificados:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Programa principal\n",
    "if __name__ == '__main__':\n",
    "    # Carregar dados\n",
    "    dados_treinamento, rotulos_treinamento_texto = carregar_dados('iris-train.data')\n",
    "    dados_teste, rotulos_teste_texto = carregar_dados('iris-test.data')\n",
    "\n",
    "    if dados_treinamento is not None and dados_teste is not None:\n",
    "        print(\"Dados carregados com sucesso!\")\n",
    "        print(f\"Número de instâncias de treinamento: {len(dados_treinamento)}\")\n",
    "        print(f\"Número de instâncias de teste: {len(dados_teste)}\")\n",
    "\n",
    "        # Normalizar dados\n",
    "        dados_treinamento_normalizados = normalizar_dados(dados_treinamento)\n",
    "        dados_teste_normalizados = normalizar_dados(dados_teste)\n",
    "        print(\"Dados normalizados com sucesso!\")\n",
    "        print(f\"Primeiras 5 instâncias de treinamento normalizadas:\\n{dados_treinamento_normalizados[:5]}\")\n",
    "        print(f\"Primeiras 5 instâncias de teste normalizadas:\\n{dados_teste_normalizados[:5]}\")\n",
    "\n",
    "        # Codificar rótulos\n",
    "        rotulos_treinamento, classes_treinamento = codificar_rotulos(rotulos_treinamento_texto)\n",
    "        rotulos_teste, classes_teste = codificar_rotulos(rotulos_teste_texto)\n",
    "        print(\"Rótulos codificados com sucesso!\")\n",
    "        print(f\"Mapeamento de classes: {classes_treinamento}\")\n",
    "        print(f\"Primeiros 5 rótulos de treinamento codificados:\\n{rotulos_treinamento[:5]}\")\n",
    "        print(f\"Primeiros 5 rótulos de teste codificados:\\n{rotulos_teste[:5]}\")\n",
    "\n",
    "        if not np.array_equal(classes_treinamento, classes_teste):\n",
    "            print(\"Aviso: As classes nos conjuntos de treinamento e teste são diferentes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b158",
   "metadata": {},
   "source": [
    "8. Instanciar a Rede MLP e Treinar\n",
    "\n",
    "Agora vamos definir a arquitetura da nossa rede e instanciá-la. A arquitetura é uma lista que especifica o número de neurônios em cada camada oculta. Para começar, podemos tentar com uma única camada oculta com um certo número de neurônios (por exemplo, 8). Para a camada de saída, teremos 3 neurônios (um para cada classe). Vamos usar a função sigmoide como função de ativação para as camadas ocultas e a função softmax para a camada de saída.\n",
    "Vamos treinar a rede usando os dados de treinamento normalizados e os rótulos codificados. Precisamos definir a taxa de aprendizado e o número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7e1efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando o treinamento da rede...\n",
      "Época 1/1000, Perda: 0.262892\n",
      "Época 2/1000, Perda: 0.238969\n",
      "Época 3/1000, Perda: 0.223975\n",
      "Época 4/1000, Perda: 0.215615\n",
      "Época 5/1000, Perda: 0.211041\n",
      "Época 6/1000, Perda: 0.208399\n",
      "Época 7/1000, Perda: 0.206715\n",
      "Época 8/1000, Perda: 0.205510\n",
      "Época 9/1000, Perda: 0.204545\n",
      "Época 10/1000, Perda: 0.203699\n",
      "Época 11/1000, Perda: 0.202911\n",
      "Época 12/1000, Perda: 0.202149\n",
      "Época 13/1000, Perda: 0.201396\n",
      "Época 14/1000, Perda: 0.200642\n",
      "Época 15/1000, Perda: 0.199883\n",
      "Época 16/1000, Perda: 0.199116\n",
      "Época 17/1000, Perda: 0.198340\n",
      "Época 18/1000, Perda: 0.197555\n",
      "Época 19/1000, Perda: 0.196760\n",
      "Época 20/1000, Perda: 0.195956\n",
      "Época 21/1000, Perda: 0.195143\n",
      "Época 22/1000, Perda: 0.194320\n",
      "Época 23/1000, Perda: 0.193489\n",
      "Época 24/1000, Perda: 0.192649\n",
      "Época 25/1000, Perda: 0.191800\n",
      "Época 26/1000, Perda: 0.190943\n",
      "Época 27/1000, Perda: 0.190079\n",
      "Época 28/1000, Perda: 0.189207\n",
      "Época 29/1000, Perda: 0.188328\n",
      "Época 30/1000, Perda: 0.187442\n",
      "Época 31/1000, Perda: 0.186549\n",
      "Época 32/1000, Perda: 0.185651\n",
      "Época 33/1000, Perda: 0.184746\n",
      "Época 34/1000, Perda: 0.183837\n",
      "Época 35/1000, Perda: 0.182923\n",
      "Época 36/1000, Perda: 0.182004\n",
      "Época 37/1000, Perda: 0.181082\n",
      "Época 38/1000, Perda: 0.180156\n",
      "Época 39/1000, Perda: 0.179227\n",
      "Época 40/1000, Perda: 0.178296\n",
      "Época 41/1000, Perda: 0.177363\n",
      "Época 42/1000, Perda: 0.176429\n",
      "Época 43/1000, Perda: 0.175494\n",
      "Época 44/1000, Perda: 0.174558\n",
      "Época 45/1000, Perda: 0.173623\n",
      "Época 46/1000, Perda: 0.172688\n",
      "Época 47/1000, Perda: 0.171755\n",
      "Época 48/1000, Perda: 0.170824\n",
      "Época 49/1000, Perda: 0.169895\n",
      "Época 50/1000, Perda: 0.168969\n",
      "Época 51/1000, Perda: 0.168046\n",
      "Época 52/1000, Perda: 0.167127\n",
      "Época 53/1000, Perda: 0.166212\n",
      "Época 54/1000, Perda: 0.165303\n",
      "Época 55/1000, Perda: 0.164398\n",
      "Época 56/1000, Perda: 0.163499\n",
      "Época 57/1000, Perda: 0.162606\n",
      "Época 58/1000, Perda: 0.161720\n",
      "Época 59/1000, Perda: 0.160841\n",
      "Época 60/1000, Perda: 0.159969\n",
      "Época 61/1000, Perda: 0.159104\n",
      "Época 62/1000, Perda: 0.158248\n",
      "Época 63/1000, Perda: 0.157399\n",
      "Época 64/1000, Perda: 0.156560\n",
      "Época 65/1000, Perda: 0.155729\n",
      "Época 66/1000, Perda: 0.154907\n",
      "Época 67/1000, Perda: 0.154095\n",
      "Época 68/1000, Perda: 0.153292\n",
      "Época 69/1000, Perda: 0.152499\n",
      "Época 70/1000, Perda: 0.151716\n",
      "Época 71/1000, Perda: 0.150943\n",
      "Época 72/1000, Perda: 0.150180\n",
      "Época 73/1000, Perda: 0.149427\n",
      "Época 74/1000, Perda: 0.148685\n",
      "Época 75/1000, Perda: 0.147954\n",
      "Época 76/1000, Perda: 0.147232\n",
      "Época 77/1000, Perda: 0.146522\n",
      "Época 78/1000, Perda: 0.145822\n",
      "Época 79/1000, Perda: 0.145133\n",
      "Época 80/1000, Perda: 0.144454\n",
      "Época 81/1000, Perda: 0.143787\n",
      "Época 82/1000, Perda: 0.143129\n",
      "Época 83/1000, Perda: 0.142483\n",
      "Época 84/1000, Perda: 0.141847\n",
      "Época 85/1000, Perda: 0.141221\n",
      "Época 86/1000, Perda: 0.140606\n",
      "Época 87/1000, Perda: 0.140001\n",
      "Época 88/1000, Perda: 0.139406\n",
      "Época 89/1000, Perda: 0.138822\n",
      "Época 90/1000, Perda: 0.138247\n",
      "Época 91/1000, Perda: 0.137683\n",
      "Época 92/1000, Perda: 0.137128\n",
      "Época 93/1000, Perda: 0.136584\n",
      "Época 94/1000, Perda: 0.136048\n",
      "Época 95/1000, Perda: 0.135522\n",
      "Época 96/1000, Perda: 0.135006\n",
      "Época 97/1000, Perda: 0.134499\n",
      "Época 98/1000, Perda: 0.134001\n",
      "Época 99/1000, Perda: 0.133511\n",
      "Época 100/1000, Perda: 0.133031\n",
      "Época 101/1000, Perda: 0.132559\n",
      "Época 102/1000, Perda: 0.132096\n",
      "Época 103/1000, Perda: 0.131641\n",
      "Época 104/1000, Perda: 0.131194\n",
      "Época 105/1000, Perda: 0.130755\n",
      "Época 106/1000, Perda: 0.130324\n",
      "Época 107/1000, Perda: 0.129901\n",
      "Época 108/1000, Perda: 0.129486\n",
      "Época 109/1000, Perda: 0.129077\n",
      "Época 110/1000, Perda: 0.128677\n",
      "Época 111/1000, Perda: 0.128283\n",
      "Época 112/1000, Perda: 0.127897\n",
      "Época 113/1000, Perda: 0.127517\n",
      "Época 114/1000, Perda: 0.127144\n",
      "Época 115/1000, Perda: 0.126777\n",
      "Época 116/1000, Perda: 0.126418\n",
      "Época 117/1000, Perda: 0.126064\n",
      "Época 118/1000, Perda: 0.125717\n",
      "Época 119/1000, Perda: 0.125375\n",
      "Época 120/1000, Perda: 0.125040\n",
      "Época 121/1000, Perda: 0.124710\n",
      "Época 122/1000, Perda: 0.124386\n",
      "Época 123/1000, Perda: 0.124068\n",
      "Época 124/1000, Perda: 0.123754\n",
      "Época 125/1000, Perda: 0.123447\n",
      "Época 126/1000, Perda: 0.123144\n",
      "Época 127/1000, Perda: 0.122847\n",
      "Época 128/1000, Perda: 0.122554\n",
      "Época 129/1000, Perda: 0.122266\n",
      "Época 130/1000, Perda: 0.121983\n",
      "Época 131/1000, Perda: 0.121705\n",
      "Época 132/1000, Perda: 0.121431\n",
      "Época 133/1000, Perda: 0.121162\n",
      "Época 134/1000, Perda: 0.120896\n",
      "Época 135/1000, Perda: 0.120635\n",
      "Época 136/1000, Perda: 0.120379\n",
      "Época 137/1000, Perda: 0.120126\n",
      "Época 138/1000, Perda: 0.119877\n",
      "Época 139/1000, Perda: 0.119632\n",
      "Época 140/1000, Perda: 0.119391\n",
      "Época 141/1000, Perda: 0.119153\n",
      "Época 142/1000, Perda: 0.118919\n",
      "Época 143/1000, Perda: 0.118688\n",
      "Época 144/1000, Perda: 0.118461\n",
      "Época 145/1000, Perda: 0.118237\n",
      "Época 146/1000, Perda: 0.118017\n",
      "Época 147/1000, Perda: 0.117799\n",
      "Época 148/1000, Perda: 0.117585\n",
      "Época 149/1000, Perda: 0.117374\n",
      "Época 150/1000, Perda: 0.117166\n",
      "Época 151/1000, Perda: 0.116961\n",
      "Época 152/1000, Perda: 0.116758\n",
      "Época 153/1000, Perda: 0.116558\n",
      "Época 154/1000, Perda: 0.116361\n",
      "Época 155/1000, Perda: 0.116167\n",
      "Época 156/1000, Perda: 0.115975\n",
      "Época 157/1000, Perda: 0.115786\n",
      "Época 158/1000, Perda: 0.115599\n",
      "Época 159/1000, Perda: 0.115415\n",
      "Época 160/1000, Perda: 0.115233\n",
      "Época 161/1000, Perda: 0.115054\n",
      "Época 162/1000, Perda: 0.114876\n",
      "Época 163/1000, Perda: 0.114701\n",
      "Época 164/1000, Perda: 0.114528\n",
      "Época 165/1000, Perda: 0.114357\n",
      "Época 166/1000, Perda: 0.114188\n",
      "Época 167/1000, Perda: 0.114022\n",
      "Época 168/1000, Perda: 0.113857\n",
      "Época 169/1000, Perda: 0.113694\n",
      "Época 170/1000, Perda: 0.113533\n",
      "Época 171/1000, Perda: 0.113374\n",
      "Época 172/1000, Perda: 0.113216\n",
      "Época 173/1000, Perda: 0.113061\n",
      "Época 174/1000, Perda: 0.112907\n",
      "Época 175/1000, Perda: 0.112755\n",
      "Época 176/1000, Perda: 0.112605\n",
      "Época 177/1000, Perda: 0.112456\n",
      "Época 178/1000, Perda: 0.112308\n",
      "Época 179/1000, Perda: 0.112163\n",
      "Época 180/1000, Perda: 0.112019\n",
      "Época 181/1000, Perda: 0.111876\n",
      "Época 182/1000, Perda: 0.111735\n",
      "Época 183/1000, Perda: 0.111595\n",
      "Época 184/1000, Perda: 0.111456\n",
      "Época 185/1000, Perda: 0.111319\n",
      "Época 186/1000, Perda: 0.111184\n",
      "Época 187/1000, Perda: 0.111049\n",
      "Época 188/1000, Perda: 0.110916\n",
      "Época 189/1000, Perda: 0.110784\n",
      "Época 190/1000, Perda: 0.110654\n",
      "Época 191/1000, Perda: 0.110524\n",
      "Época 192/1000, Perda: 0.110396\n",
      "Época 193/1000, Perda: 0.110269\n",
      "Época 194/1000, Perda: 0.110143\n",
      "Época 195/1000, Perda: 0.110018\n",
      "Época 196/1000, Perda: 0.109895\n",
      "Época 197/1000, Perda: 0.109772\n",
      "Época 198/1000, Perda: 0.109650\n",
      "Época 199/1000, Perda: 0.109530\n",
      "Época 200/1000, Perda: 0.109410\n",
      "Época 201/1000, Perda: 0.109292\n",
      "Época 202/1000, Perda: 0.109174\n",
      "Época 203/1000, Perda: 0.109057\n",
      "Época 204/1000, Perda: 0.108942\n",
      "Época 205/1000, Perda: 0.108827\n",
      "Época 206/1000, Perda: 0.108713\n",
      "Época 207/1000, Perda: 0.108600\n",
      "Época 208/1000, Perda: 0.108488\n",
      "Época 209/1000, Perda: 0.108376\n",
      "Época 210/1000, Perda: 0.108266\n",
      "Época 211/1000, Perda: 0.108156\n",
      "Época 212/1000, Perda: 0.108047\n",
      "Época 213/1000, Perda: 0.107939\n",
      "Época 214/1000, Perda: 0.107832\n",
      "Época 215/1000, Perda: 0.107725\n",
      "Época 216/1000, Perda: 0.107619\n",
      "Época 217/1000, Perda: 0.107514\n",
      "Época 218/1000, Perda: 0.107410\n",
      "Época 219/1000, Perda: 0.107306\n",
      "Época 220/1000, Perda: 0.107203\n",
      "Época 221/1000, Perda: 0.107101\n",
      "Época 222/1000, Perda: 0.106999\n",
      "Época 223/1000, Perda: 0.106898\n",
      "Época 224/1000, Perda: 0.106798\n",
      "Época 225/1000, Perda: 0.106698\n",
      "Época 226/1000, Perda: 0.106599\n",
      "Época 227/1000, Perda: 0.106500\n",
      "Época 228/1000, Perda: 0.106402\n",
      "Época 229/1000, Perda: 0.106305\n",
      "Época 230/1000, Perda: 0.106208\n",
      "Época 231/1000, Perda: 0.106112\n",
      "Época 232/1000, Perda: 0.106017\n",
      "Época 233/1000, Perda: 0.105921\n",
      "Época 234/1000, Perda: 0.105827\n",
      "Época 235/1000, Perda: 0.105733\n",
      "Época 236/1000, Perda: 0.105639\n",
      "Época 237/1000, Perda: 0.105546\n",
      "Época 238/1000, Perda: 0.105454\n",
      "Época 239/1000, Perda: 0.105362\n",
      "Época 240/1000, Perda: 0.105270\n",
      "Época 241/1000, Perda: 0.105179\n",
      "Época 242/1000, Perda: 0.105089\n",
      "Época 243/1000, Perda: 0.104999\n",
      "Época 244/1000, Perda: 0.104909\n",
      "Época 245/1000, Perda: 0.104820\n",
      "Época 246/1000, Perda: 0.104731\n",
      "Época 247/1000, Perda: 0.104643\n",
      "Época 248/1000, Perda: 0.104555\n",
      "Época 249/1000, Perda: 0.104468\n",
      "Época 250/1000, Perda: 0.104380\n",
      "Época 251/1000, Perda: 0.104294\n",
      "Época 252/1000, Perda: 0.104208\n",
      "Época 253/1000, Perda: 0.104122\n",
      "Época 254/1000, Perda: 0.104036\n",
      "Época 255/1000, Perda: 0.103951\n",
      "Época 256/1000, Perda: 0.103866\n",
      "Época 257/1000, Perda: 0.103782\n",
      "Época 258/1000, Perda: 0.103698\n",
      "Época 259/1000, Perda: 0.103615\n",
      "Época 260/1000, Perda: 0.103531\n",
      "Época 261/1000, Perda: 0.103448\n",
      "Época 262/1000, Perda: 0.103366\n",
      "Época 263/1000, Perda: 0.103284\n",
      "Época 264/1000, Perda: 0.103202\n",
      "Época 265/1000, Perda: 0.103120\n",
      "Época 266/1000, Perda: 0.103039\n",
      "Época 267/1000, Perda: 0.102958\n",
      "Época 268/1000, Perda: 0.102877\n",
      "Época 269/1000, Perda: 0.102797\n",
      "Época 270/1000, Perda: 0.102717\n",
      "Época 271/1000, Perda: 0.102637\n",
      "Época 272/1000, Perda: 0.102558\n",
      "Época 273/1000, Perda: 0.102479\n",
      "Época 274/1000, Perda: 0.102400\n",
      "Época 275/1000, Perda: 0.102321\n",
      "Época 276/1000, Perda: 0.102243\n",
      "Época 277/1000, Perda: 0.102165\n",
      "Época 278/1000, Perda: 0.102087\n",
      "Época 279/1000, Perda: 0.102010\n",
      "Época 280/1000, Perda: 0.101933\n",
      "Época 281/1000, Perda: 0.101856\n",
      "Época 282/1000, Perda: 0.101779\n",
      "Época 283/1000, Perda: 0.101703\n",
      "Época 284/1000, Perda: 0.101626\n",
      "Época 285/1000, Perda: 0.101550\n",
      "Época 286/1000, Perda: 0.101475\n",
      "Época 287/1000, Perda: 0.101399\n",
      "Época 288/1000, Perda: 0.101324\n",
      "Época 289/1000, Perda: 0.101249\n",
      "Época 290/1000, Perda: 0.101174\n",
      "Época 291/1000, Perda: 0.101100\n",
      "Época 292/1000, Perda: 0.101025\n",
      "Época 293/1000, Perda: 0.100951\n",
      "Época 294/1000, Perda: 0.100877\n",
      "Época 295/1000, Perda: 0.100804\n",
      "Época 296/1000, Perda: 0.100730\n",
      "Época 297/1000, Perda: 0.100657\n",
      "Época 298/1000, Perda: 0.100584\n",
      "Época 299/1000, Perda: 0.100511\n",
      "Época 300/1000, Perda: 0.100438\n",
      "Época 301/1000, Perda: 0.100366\n",
      "Época 302/1000, Perda: 0.100293\n",
      "Época 303/1000, Perda: 0.100221\n",
      "Época 304/1000, Perda: 0.100149\n",
      "Época 305/1000, Perda: 0.100077\n",
      "Época 306/1000, Perda: 0.100006\n",
      "Época 307/1000, Perda: 0.099934\n",
      "Época 308/1000, Perda: 0.099863\n",
      "Época 309/1000, Perda: 0.099792\n",
      "Época 310/1000, Perda: 0.099721\n",
      "Época 311/1000, Perda: 0.099651\n",
      "Época 312/1000, Perda: 0.099580\n",
      "Época 313/1000, Perda: 0.099510\n",
      "Época 314/1000, Perda: 0.099439\n",
      "Época 315/1000, Perda: 0.099369\n",
      "Época 316/1000, Perda: 0.099299\n",
      "Época 317/1000, Perda: 0.099230\n",
      "Época 318/1000, Perda: 0.099160\n",
      "Época 319/1000, Perda: 0.099091\n",
      "Época 320/1000, Perda: 0.099021\n",
      "Época 321/1000, Perda: 0.098952\n",
      "Época 322/1000, Perda: 0.098883\n",
      "Época 323/1000, Perda: 0.098814\n",
      "Época 324/1000, Perda: 0.098745\n",
      "Época 325/1000, Perda: 0.098677\n",
      "Época 326/1000, Perda: 0.098608\n",
      "Época 327/1000, Perda: 0.098540\n",
      "Época 328/1000, Perda: 0.098472\n",
      "Época 329/1000, Perda: 0.098404\n",
      "Época 330/1000, Perda: 0.098336\n",
      "Época 331/1000, Perda: 0.098268\n",
      "Época 332/1000, Perda: 0.098200\n",
      "Época 333/1000, Perda: 0.098132\n",
      "Época 334/1000, Perda: 0.098065\n",
      "Época 335/1000, Perda: 0.097997\n",
      "Época 336/1000, Perda: 0.097930\n",
      "Época 337/1000, Perda: 0.097863\n",
      "Época 338/1000, Perda: 0.097796\n",
      "Época 339/1000, Perda: 0.097729\n",
      "Época 340/1000, Perda: 0.097662\n",
      "Época 341/1000, Perda: 0.097595\n",
      "Época 342/1000, Perda: 0.097529\n",
      "Época 343/1000, Perda: 0.097462\n",
      "Época 344/1000, Perda: 0.097396\n",
      "Época 345/1000, Perda: 0.097330\n",
      "Época 346/1000, Perda: 0.097263\n",
      "Época 347/1000, Perda: 0.097197\n",
      "Época 348/1000, Perda: 0.097131\n",
      "Época 349/1000, Perda: 0.097065\n",
      "Época 350/1000, Perda: 0.096999\n",
      "Época 351/1000, Perda: 0.096934\n",
      "Época 352/1000, Perda: 0.096868\n",
      "Época 353/1000, Perda: 0.096802\n",
      "Época 354/1000, Perda: 0.096737\n",
      "Época 355/1000, Perda: 0.096671\n",
      "Época 356/1000, Perda: 0.096606\n",
      "Época 357/1000, Perda: 0.096541\n",
      "Época 358/1000, Perda: 0.096475\n",
      "Época 359/1000, Perda: 0.096410\n",
      "Época 360/1000, Perda: 0.096345\n",
      "Época 361/1000, Perda: 0.096280\n",
      "Época 362/1000, Perda: 0.096215\n",
      "Época 363/1000, Perda: 0.096150\n",
      "Época 364/1000, Perda: 0.096086\n",
      "Época 365/1000, Perda: 0.096021\n",
      "Época 366/1000, Perda: 0.095956\n",
      "Época 367/1000, Perda: 0.095892\n",
      "Época 368/1000, Perda: 0.095827\n",
      "Época 369/1000, Perda: 0.095763\n",
      "Época 370/1000, Perda: 0.095698\n",
      "Época 371/1000, Perda: 0.095634\n",
      "Época 372/1000, Perda: 0.095569\n",
      "Época 373/1000, Perda: 0.095505\n",
      "Época 374/1000, Perda: 0.095441\n",
      "Época 375/1000, Perda: 0.095377\n",
      "Época 376/1000, Perda: 0.095313\n",
      "Época 377/1000, Perda: 0.095249\n",
      "Época 378/1000, Perda: 0.095185\n",
      "Época 379/1000, Perda: 0.095121\n",
      "Época 380/1000, Perda: 0.095057\n",
      "Época 381/1000, Perda: 0.094993\n",
      "Época 382/1000, Perda: 0.094929\n",
      "Época 383/1000, Perda: 0.094865\n",
      "Época 384/1000, Perda: 0.094802\n",
      "Época 385/1000, Perda: 0.094738\n",
      "Época 386/1000, Perda: 0.094674\n",
      "Época 387/1000, Perda: 0.094611\n",
      "Época 388/1000, Perda: 0.094547\n",
      "Época 389/1000, Perda: 0.094483\n",
      "Época 390/1000, Perda: 0.094420\n",
      "Época 391/1000, Perda: 0.094356\n",
      "Época 392/1000, Perda: 0.094293\n",
      "Época 393/1000, Perda: 0.094229\n",
      "Época 394/1000, Perda: 0.094166\n",
      "Época 395/1000, Perda: 0.094102\n",
      "Época 396/1000, Perda: 0.094039\n",
      "Época 397/1000, Perda: 0.093976\n",
      "Época 398/1000, Perda: 0.093912\n",
      "Época 399/1000, Perda: 0.093849\n",
      "Época 400/1000, Perda: 0.093786\n",
      "Época 401/1000, Perda: 0.093722\n",
      "Época 402/1000, Perda: 0.093659\n",
      "Época 403/1000, Perda: 0.093596\n",
      "Época 404/1000, Perda: 0.093533\n",
      "Época 405/1000, Perda: 0.093469\n",
      "Época 406/1000, Perda: 0.093406\n",
      "Época 407/1000, Perda: 0.093343\n",
      "Época 408/1000, Perda: 0.093280\n",
      "Época 409/1000, Perda: 0.093217\n",
      "Época 410/1000, Perda: 0.093153\n",
      "Época 411/1000, Perda: 0.093090\n",
      "Época 412/1000, Perda: 0.093027\n",
      "Época 413/1000, Perda: 0.092964\n",
      "Época 414/1000, Perda: 0.092901\n",
      "Época 415/1000, Perda: 0.092838\n",
      "Época 416/1000, Perda: 0.092774\n",
      "Época 417/1000, Perda: 0.092711\n",
      "Época 418/1000, Perda: 0.092648\n",
      "Época 419/1000, Perda: 0.092585\n",
      "Época 420/1000, Perda: 0.092522\n",
      "Época 421/1000, Perda: 0.092459\n",
      "Época 422/1000, Perda: 0.092395\n",
      "Época 423/1000, Perda: 0.092332\n",
      "Época 424/1000, Perda: 0.092269\n",
      "Época 425/1000, Perda: 0.092206\n",
      "Época 426/1000, Perda: 0.092143\n",
      "Época 427/1000, Perda: 0.092079\n",
      "Época 428/1000, Perda: 0.092016\n",
      "Época 429/1000, Perda: 0.091953\n",
      "Época 430/1000, Perda: 0.091890\n",
      "Época 431/1000, Perda: 0.091826\n",
      "Época 432/1000, Perda: 0.091763\n",
      "Época 433/1000, Perda: 0.091700\n",
      "Época 434/1000, Perda: 0.091636\n",
      "Época 435/1000, Perda: 0.091573\n",
      "Época 436/1000, Perda: 0.091510\n",
      "Época 437/1000, Perda: 0.091446\n",
      "Época 438/1000, Perda: 0.091383\n",
      "Época 439/1000, Perda: 0.091319\n",
      "Época 440/1000, Perda: 0.091256\n",
      "Época 441/1000, Perda: 0.091192\n",
      "Época 442/1000, Perda: 0.091129\n",
      "Época 443/1000, Perda: 0.091065\n",
      "Época 444/1000, Perda: 0.091002\n",
      "Época 445/1000, Perda: 0.090938\n",
      "Época 446/1000, Perda: 0.090874\n",
      "Época 447/1000, Perda: 0.090811\n",
      "Época 448/1000, Perda: 0.090747\n",
      "Época 449/1000, Perda: 0.090683\n",
      "Época 450/1000, Perda: 0.090619\n",
      "Época 451/1000, Perda: 0.090556\n",
      "Época 452/1000, Perda: 0.090492\n",
      "Época 453/1000, Perda: 0.090428\n",
      "Época 454/1000, Perda: 0.090364\n",
      "Época 455/1000, Perda: 0.090300\n",
      "Época 456/1000, Perda: 0.090236\n",
      "Época 457/1000, Perda: 0.090172\n",
      "Época 458/1000, Perda: 0.090108\n",
      "Época 459/1000, Perda: 0.090044\n",
      "Época 460/1000, Perda: 0.089979\n",
      "Época 461/1000, Perda: 0.089915\n",
      "Época 462/1000, Perda: 0.089851\n",
      "Época 463/1000, Perda: 0.089787\n",
      "Época 464/1000, Perda: 0.089722\n",
      "Época 465/1000, Perda: 0.089658\n",
      "Época 466/1000, Perda: 0.089593\n",
      "Época 467/1000, Perda: 0.089529\n",
      "Época 468/1000, Perda: 0.089464\n",
      "Época 469/1000, Perda: 0.089400\n",
      "Época 470/1000, Perda: 0.089335\n",
      "Época 471/1000, Perda: 0.089270\n",
      "Época 472/1000, Perda: 0.089206\n",
      "Época 473/1000, Perda: 0.089141\n",
      "Época 474/1000, Perda: 0.089076\n",
      "Época 475/1000, Perda: 0.089011\n",
      "Época 476/1000, Perda: 0.088946\n",
      "Época 477/1000, Perda: 0.088881\n",
      "Época 478/1000, Perda: 0.088816\n",
      "Época 479/1000, Perda: 0.088751\n",
      "Época 480/1000, Perda: 0.088686\n",
      "Época 481/1000, Perda: 0.088620\n",
      "Época 482/1000, Perda: 0.088555\n",
      "Época 483/1000, Perda: 0.088490\n",
      "Época 484/1000, Perda: 0.088424\n",
      "Época 485/1000, Perda: 0.088359\n",
      "Época 486/1000, Perda: 0.088293\n",
      "Época 487/1000, Perda: 0.088227\n",
      "Época 488/1000, Perda: 0.088162\n",
      "Época 489/1000, Perda: 0.088096\n",
      "Época 490/1000, Perda: 0.088030\n",
      "Época 491/1000, Perda: 0.087964\n",
      "Época 492/1000, Perda: 0.087898\n",
      "Época 493/1000, Perda: 0.087832\n",
      "Época 494/1000, Perda: 0.087766\n",
      "Época 495/1000, Perda: 0.087700\n",
      "Época 496/1000, Perda: 0.087633\n",
      "Época 497/1000, Perda: 0.087567\n",
      "Época 498/1000, Perda: 0.087501\n",
      "Época 499/1000, Perda: 0.087434\n",
      "Época 500/1000, Perda: 0.087368\n",
      "Época 501/1000, Perda: 0.087301\n",
      "Época 502/1000, Perda: 0.087234\n",
      "Época 503/1000, Perda: 0.087168\n",
      "Época 504/1000, Perda: 0.087101\n",
      "Época 505/1000, Perda: 0.087034\n",
      "Época 506/1000, Perda: 0.086967\n",
      "Época 507/1000, Perda: 0.086900\n",
      "Época 508/1000, Perda: 0.086832\n",
      "Época 509/1000, Perda: 0.086765\n",
      "Época 510/1000, Perda: 0.086698\n",
      "Época 511/1000, Perda: 0.086630\n",
      "Época 512/1000, Perda: 0.086563\n",
      "Época 513/1000, Perda: 0.086495\n",
      "Época 514/1000, Perda: 0.086428\n",
      "Época 515/1000, Perda: 0.086360\n",
      "Época 516/1000, Perda: 0.086292\n",
      "Época 517/1000, Perda: 0.086224\n",
      "Época 518/1000, Perda: 0.086156\n",
      "Época 519/1000, Perda: 0.086088\n",
      "Época 520/1000, Perda: 0.086020\n",
      "Época 521/1000, Perda: 0.085952\n",
      "Época 522/1000, Perda: 0.085884\n",
      "Época 523/1000, Perda: 0.085815\n",
      "Época 524/1000, Perda: 0.085747\n",
      "Época 525/1000, Perda: 0.085678\n",
      "Época 526/1000, Perda: 0.085609\n",
      "Época 527/1000, Perda: 0.085541\n",
      "Época 528/1000, Perda: 0.085472\n",
      "Época 529/1000, Perda: 0.085403\n",
      "Época 530/1000, Perda: 0.085334\n",
      "Época 531/1000, Perda: 0.085265\n",
      "Época 532/1000, Perda: 0.085195\n",
      "Época 533/1000, Perda: 0.085126\n",
      "Época 534/1000, Perda: 0.085057\n",
      "Época 535/1000, Perda: 0.084987\n",
      "Época 536/1000, Perda: 0.084918\n",
      "Época 537/1000, Perda: 0.084848\n",
      "Época 538/1000, Perda: 0.084778\n",
      "Época 539/1000, Perda: 0.084708\n",
      "Época 540/1000, Perda: 0.084639\n",
      "Época 541/1000, Perda: 0.084569\n",
      "Época 542/1000, Perda: 0.084498\n",
      "Época 543/1000, Perda: 0.084428\n",
      "Época 544/1000, Perda: 0.084358\n",
      "Época 545/1000, Perda: 0.084287\n",
      "Época 546/1000, Perda: 0.084217\n",
      "Época 547/1000, Perda: 0.084146\n",
      "Época 548/1000, Perda: 0.084076\n",
      "Época 549/1000, Perda: 0.084005\n",
      "Época 550/1000, Perda: 0.083934\n",
      "Época 551/1000, Perda: 0.083863\n",
      "Época 552/1000, Perda: 0.083792\n",
      "Época 553/1000, Perda: 0.083721\n",
      "Época 554/1000, Perda: 0.083649\n",
      "Época 555/1000, Perda: 0.083578\n",
      "Época 556/1000, Perda: 0.083507\n",
      "Época 557/1000, Perda: 0.083435\n",
      "Época 558/1000, Perda: 0.083363\n",
      "Época 559/1000, Perda: 0.083292\n",
      "Época 560/1000, Perda: 0.083220\n",
      "Época 561/1000, Perda: 0.083148\n",
      "Época 562/1000, Perda: 0.083076\n",
      "Época 563/1000, Perda: 0.083003\n",
      "Época 564/1000, Perda: 0.082931\n",
      "Época 565/1000, Perda: 0.082859\n",
      "Época 566/1000, Perda: 0.082786\n",
      "Época 567/1000, Perda: 0.082714\n",
      "Época 568/1000, Perda: 0.082641\n",
      "Época 569/1000, Perda: 0.082568\n",
      "Época 570/1000, Perda: 0.082495\n",
      "Época 571/1000, Perda: 0.082422\n",
      "Época 572/1000, Perda: 0.082349\n",
      "Época 573/1000, Perda: 0.082276\n",
      "Época 574/1000, Perda: 0.082203\n",
      "Época 575/1000, Perda: 0.082130\n",
      "Época 576/1000, Perda: 0.082056\n",
      "Época 577/1000, Perda: 0.081983\n",
      "Época 578/1000, Perda: 0.081909\n",
      "Época 579/1000, Perda: 0.081835\n",
      "Época 580/1000, Perda: 0.081761\n",
      "Época 581/1000, Perda: 0.081687\n",
      "Época 582/1000, Perda: 0.081613\n",
      "Época 583/1000, Perda: 0.081539\n",
      "Época 584/1000, Perda: 0.081465\n",
      "Época 585/1000, Perda: 0.081390\n",
      "Época 586/1000, Perda: 0.081316\n",
      "Época 587/1000, Perda: 0.081241\n",
      "Época 588/1000, Perda: 0.081167\n",
      "Época 589/1000, Perda: 0.081092\n",
      "Época 590/1000, Perda: 0.081017\n",
      "Época 591/1000, Perda: 0.080942\n",
      "Época 592/1000, Perda: 0.080867\n",
      "Época 593/1000, Perda: 0.080792\n",
      "Época 594/1000, Perda: 0.080716\n",
      "Época 595/1000, Perda: 0.080641\n",
      "Época 596/1000, Perda: 0.080565\n",
      "Época 597/1000, Perda: 0.080490\n",
      "Época 598/1000, Perda: 0.080414\n",
      "Época 599/1000, Perda: 0.080338\n",
      "Época 600/1000, Perda: 0.080262\n",
      "Época 601/1000, Perda: 0.080186\n",
      "Época 602/1000, Perda: 0.080110\n",
      "Época 603/1000, Perda: 0.080034\n",
      "Época 604/1000, Perda: 0.079958\n",
      "Época 605/1000, Perda: 0.079881\n",
      "Época 606/1000, Perda: 0.079805\n",
      "Época 607/1000, Perda: 0.079728\n",
      "Época 608/1000, Perda: 0.079652\n",
      "Época 609/1000, Perda: 0.079575\n",
      "Época 610/1000, Perda: 0.079498\n",
      "Época 611/1000, Perda: 0.079421\n",
      "Época 612/1000, Perda: 0.079344\n",
      "Época 613/1000, Perda: 0.079267\n",
      "Época 614/1000, Perda: 0.079189\n",
      "Época 615/1000, Perda: 0.079112\n",
      "Época 616/1000, Perda: 0.079034\n",
      "Época 617/1000, Perda: 0.078957\n",
      "Época 618/1000, Perda: 0.078879\n",
      "Época 619/1000, Perda: 0.078801\n",
      "Época 620/1000, Perda: 0.078724\n",
      "Época 621/1000, Perda: 0.078646\n",
      "Época 622/1000, Perda: 0.078567\n",
      "Época 623/1000, Perda: 0.078489\n",
      "Época 624/1000, Perda: 0.078411\n",
      "Época 625/1000, Perda: 0.078333\n",
      "Época 626/1000, Perda: 0.078254\n",
      "Época 627/1000, Perda: 0.078176\n",
      "Época 628/1000, Perda: 0.078097\n",
      "Época 629/1000, Perda: 0.078018\n",
      "Época 630/1000, Perda: 0.077939\n",
      "Época 631/1000, Perda: 0.077861\n",
      "Época 632/1000, Perda: 0.077782\n",
      "Época 633/1000, Perda: 0.077702\n",
      "Época 634/1000, Perda: 0.077623\n",
      "Época 635/1000, Perda: 0.077544\n",
      "Época 636/1000, Perda: 0.077465\n",
      "Época 637/1000, Perda: 0.077385\n",
      "Época 638/1000, Perda: 0.077306\n",
      "Época 639/1000, Perda: 0.077226\n",
      "Época 640/1000, Perda: 0.077146\n",
      "Época 641/1000, Perda: 0.077066\n",
      "Época 642/1000, Perda: 0.076986\n",
      "Época 643/1000, Perda: 0.076906\n",
      "Época 644/1000, Perda: 0.076826\n",
      "Época 645/1000, Perda: 0.076746\n",
      "Época 646/1000, Perda: 0.076666\n",
      "Época 647/1000, Perda: 0.076585\n",
      "Época 648/1000, Perda: 0.076505\n",
      "Época 649/1000, Perda: 0.076424\n",
      "Época 650/1000, Perda: 0.076344\n",
      "Época 651/1000, Perda: 0.076263\n",
      "Época 652/1000, Perda: 0.076182\n",
      "Época 653/1000, Perda: 0.076101\n",
      "Época 654/1000, Perda: 0.076020\n",
      "Época 655/1000, Perda: 0.075939\n",
      "Época 656/1000, Perda: 0.075858\n",
      "Época 657/1000, Perda: 0.075777\n",
      "Época 658/1000, Perda: 0.075696\n",
      "Época 659/1000, Perda: 0.075614\n",
      "Época 660/1000, Perda: 0.075533\n",
      "Época 661/1000, Perda: 0.075451\n",
      "Época 662/1000, Perda: 0.075370\n",
      "Época 663/1000, Perda: 0.075288\n",
      "Época 664/1000, Perda: 0.075206\n",
      "Época 665/1000, Perda: 0.075125\n",
      "Época 666/1000, Perda: 0.075043\n",
      "Época 667/1000, Perda: 0.074961\n",
      "Época 668/1000, Perda: 0.074879\n",
      "Época 669/1000, Perda: 0.074796\n",
      "Época 670/1000, Perda: 0.074714\n",
      "Época 671/1000, Perda: 0.074632\n",
      "Época 672/1000, Perda: 0.074549\n",
      "Época 673/1000, Perda: 0.074467\n",
      "Época 674/1000, Perda: 0.074385\n",
      "Época 675/1000, Perda: 0.074302\n",
      "Época 676/1000, Perda: 0.074219\n",
      "Época 677/1000, Perda: 0.074137\n",
      "Época 678/1000, Perda: 0.074054\n",
      "Época 679/1000, Perda: 0.073971\n",
      "Época 680/1000, Perda: 0.073888\n",
      "Época 681/1000, Perda: 0.073805\n",
      "Época 682/1000, Perda: 0.073722\n",
      "Época 683/1000, Perda: 0.073639\n",
      "Época 684/1000, Perda: 0.073556\n",
      "Época 685/1000, Perda: 0.073472\n",
      "Época 686/1000, Perda: 0.073389\n",
      "Época 687/1000, Perda: 0.073306\n",
      "Época 688/1000, Perda: 0.073222\n",
      "Época 689/1000, Perda: 0.073139\n",
      "Época 690/1000, Perda: 0.073055\n",
      "Época 691/1000, Perda: 0.072971\n",
      "Época 692/1000, Perda: 0.072888\n",
      "Época 693/1000, Perda: 0.072804\n",
      "Época 694/1000, Perda: 0.072720\n",
      "Época 695/1000, Perda: 0.072636\n",
      "Época 696/1000, Perda: 0.072552\n",
      "Época 697/1000, Perda: 0.072468\n",
      "Época 698/1000, Perda: 0.072384\n",
      "Época 699/1000, Perda: 0.072300\n",
      "Época 700/1000, Perda: 0.072216\n",
      "Época 701/1000, Perda: 0.072132\n",
      "Época 702/1000, Perda: 0.072048\n",
      "Época 703/1000, Perda: 0.071963\n",
      "Época 704/1000, Perda: 0.071879\n",
      "Época 705/1000, Perda: 0.071794\n",
      "Época 706/1000, Perda: 0.071710\n",
      "Época 707/1000, Perda: 0.071625\n",
      "Época 708/1000, Perda: 0.071541\n",
      "Época 709/1000, Perda: 0.071456\n",
      "Época 710/1000, Perda: 0.071372\n",
      "Época 711/1000, Perda: 0.071287\n",
      "Época 712/1000, Perda: 0.071202\n",
      "Época 713/1000, Perda: 0.071117\n",
      "Época 714/1000, Perda: 0.071033\n",
      "Época 715/1000, Perda: 0.070948\n",
      "Época 716/1000, Perda: 0.070863\n",
      "Época 717/1000, Perda: 0.070778\n",
      "Época 718/1000, Perda: 0.070693\n",
      "Época 719/1000, Perda: 0.070608\n",
      "Época 720/1000, Perda: 0.070523\n",
      "Época 721/1000, Perda: 0.070438\n",
      "Época 722/1000, Perda: 0.070353\n",
      "Época 723/1000, Perda: 0.070267\n",
      "Época 724/1000, Perda: 0.070182\n",
      "Época 725/1000, Perda: 0.070097\n",
      "Época 726/1000, Perda: 0.070012\n",
      "Época 727/1000, Perda: 0.069926\n",
      "Época 728/1000, Perda: 0.069841\n",
      "Época 729/1000, Perda: 0.069756\n",
      "Época 730/1000, Perda: 0.069670\n",
      "Época 731/1000, Perda: 0.069585\n",
      "Época 732/1000, Perda: 0.069499\n",
      "Época 733/1000, Perda: 0.069414\n",
      "Época 734/1000, Perda: 0.069328\n",
      "Época 735/1000, Perda: 0.069243\n",
      "Época 736/1000, Perda: 0.069157\n",
      "Época 737/1000, Perda: 0.069072\n",
      "Época 738/1000, Perda: 0.068986\n",
      "Época 739/1000, Perda: 0.068900\n",
      "Época 740/1000, Perda: 0.068815\n",
      "Época 741/1000, Perda: 0.068729\n",
      "Época 742/1000, Perda: 0.068643\n",
      "Época 743/1000, Perda: 0.068558\n",
      "Época 744/1000, Perda: 0.068472\n",
      "Época 745/1000, Perda: 0.068386\n",
      "Época 746/1000, Perda: 0.068301\n",
      "Época 747/1000, Perda: 0.068215\n",
      "Época 748/1000, Perda: 0.068129\n",
      "Época 749/1000, Perda: 0.068043\n",
      "Época 750/1000, Perda: 0.067957\n",
      "Época 751/1000, Perda: 0.067872\n",
      "Época 752/1000, Perda: 0.067786\n",
      "Época 753/1000, Perda: 0.067700\n",
      "Época 754/1000, Perda: 0.067614\n",
      "Época 755/1000, Perda: 0.067528\n",
      "Época 756/1000, Perda: 0.067442\n",
      "Época 757/1000, Perda: 0.067356\n",
      "Época 758/1000, Perda: 0.067271\n",
      "Época 759/1000, Perda: 0.067185\n",
      "Época 760/1000, Perda: 0.067099\n",
      "Época 761/1000, Perda: 0.067013\n",
      "Época 762/1000, Perda: 0.066927\n",
      "Época 763/1000, Perda: 0.066841\n",
      "Época 764/1000, Perda: 0.066755\n",
      "Época 765/1000, Perda: 0.066670\n",
      "Época 766/1000, Perda: 0.066584\n",
      "Época 767/1000, Perda: 0.066498\n",
      "Época 768/1000, Perda: 0.066412\n",
      "Época 769/1000, Perda: 0.066326\n",
      "Época 770/1000, Perda: 0.066240\n",
      "Época 771/1000, Perda: 0.066154\n",
      "Época 772/1000, Perda: 0.066069\n",
      "Época 773/1000, Perda: 0.065983\n",
      "Época 774/1000, Perda: 0.065897\n",
      "Época 775/1000, Perda: 0.065811\n",
      "Época 776/1000, Perda: 0.065725\n",
      "Época 777/1000, Perda: 0.065640\n",
      "Época 778/1000, Perda: 0.065554\n",
      "Época 779/1000, Perda: 0.065468\n",
      "Época 780/1000, Perda: 0.065382\n",
      "Época 781/1000, Perda: 0.065297\n",
      "Época 782/1000, Perda: 0.065211\n",
      "Época 783/1000, Perda: 0.065125\n",
      "Época 784/1000, Perda: 0.065039\n",
      "Época 785/1000, Perda: 0.064954\n",
      "Época 786/1000, Perda: 0.064868\n",
      "Época 787/1000, Perda: 0.064783\n",
      "Época 788/1000, Perda: 0.064697\n",
      "Época 789/1000, Perda: 0.064611\n",
      "Época 790/1000, Perda: 0.064526\n",
      "Época 791/1000, Perda: 0.064440\n",
      "Época 792/1000, Perda: 0.064355\n",
      "Época 793/1000, Perda: 0.064269\n",
      "Época 794/1000, Perda: 0.064184\n",
      "Época 795/1000, Perda: 0.064099\n",
      "Época 796/1000, Perda: 0.064013\n",
      "Época 797/1000, Perda: 0.063928\n",
      "Época 798/1000, Perda: 0.063843\n",
      "Época 799/1000, Perda: 0.063757\n",
      "Época 800/1000, Perda: 0.063672\n",
      "Época 801/1000, Perda: 0.063587\n",
      "Época 802/1000, Perda: 0.063502\n",
      "Época 803/1000, Perda: 0.063417\n",
      "Época 804/1000, Perda: 0.063331\n",
      "Época 805/1000, Perda: 0.063246\n",
      "Época 806/1000, Perda: 0.063161\n",
      "Época 807/1000, Perda: 0.063076\n",
      "Época 808/1000, Perda: 0.062991\n",
      "Época 809/1000, Perda: 0.062907\n",
      "Época 810/1000, Perda: 0.062822\n",
      "Época 811/1000, Perda: 0.062737\n",
      "Época 812/1000, Perda: 0.062652\n",
      "Época 813/1000, Perda: 0.062567\n",
      "Época 814/1000, Perda: 0.062483\n",
      "Época 815/1000, Perda: 0.062398\n",
      "Época 816/1000, Perda: 0.062313\n",
      "Época 817/1000, Perda: 0.062229\n",
      "Época 818/1000, Perda: 0.062144\n",
      "Época 819/1000, Perda: 0.062060\n",
      "Época 820/1000, Perda: 0.061976\n",
      "Época 821/1000, Perda: 0.061891\n",
      "Época 822/1000, Perda: 0.061807\n",
      "Época 823/1000, Perda: 0.061723\n",
      "Época 824/1000, Perda: 0.061638\n",
      "Época 825/1000, Perda: 0.061554\n",
      "Época 826/1000, Perda: 0.061470\n",
      "Época 827/1000, Perda: 0.061386\n",
      "Época 828/1000, Perda: 0.061302\n",
      "Época 829/1000, Perda: 0.061218\n",
      "Época 830/1000, Perda: 0.061134\n",
      "Época 831/1000, Perda: 0.061051\n",
      "Época 832/1000, Perda: 0.060967\n",
      "Época 833/1000, Perda: 0.060883\n",
      "Época 834/1000, Perda: 0.060800\n",
      "Época 835/1000, Perda: 0.060716\n",
      "Época 836/1000, Perda: 0.060632\n",
      "Época 837/1000, Perda: 0.060549\n",
      "Época 838/1000, Perda: 0.060466\n",
      "Época 839/1000, Perda: 0.060382\n",
      "Época 840/1000, Perda: 0.060299\n",
      "Época 841/1000, Perda: 0.060216\n",
      "Época 842/1000, Perda: 0.060133\n",
      "Época 843/1000, Perda: 0.060050\n",
      "Época 844/1000, Perda: 0.059967\n",
      "Época 845/1000, Perda: 0.059884\n",
      "Época 846/1000, Perda: 0.059801\n",
      "Época 847/1000, Perda: 0.059718\n",
      "Época 848/1000, Perda: 0.059636\n",
      "Época 849/1000, Perda: 0.059553\n",
      "Época 850/1000, Perda: 0.059470\n",
      "Época 851/1000, Perda: 0.059388\n",
      "Época 852/1000, Perda: 0.059306\n",
      "Época 853/1000, Perda: 0.059223\n",
      "Época 854/1000, Perda: 0.059141\n",
      "Época 855/1000, Perda: 0.059059\n",
      "Época 856/1000, Perda: 0.058977\n",
      "Época 857/1000, Perda: 0.058895\n",
      "Época 858/1000, Perda: 0.058813\n",
      "Época 859/1000, Perda: 0.058731\n",
      "Época 860/1000, Perda: 0.058649\n",
      "Época 861/1000, Perda: 0.058567\n",
      "Época 862/1000, Perda: 0.058486\n",
      "Época 863/1000, Perda: 0.058404\n",
      "Época 864/1000, Perda: 0.058323\n",
      "Época 865/1000, Perda: 0.058242\n",
      "Época 866/1000, Perda: 0.058160\n",
      "Época 867/1000, Perda: 0.058079\n",
      "Época 868/1000, Perda: 0.057998\n",
      "Época 869/1000, Perda: 0.057917\n",
      "Época 870/1000, Perda: 0.057836\n",
      "Época 871/1000, Perda: 0.057755\n",
      "Época 872/1000, Perda: 0.057674\n",
      "Época 873/1000, Perda: 0.057594\n",
      "Época 874/1000, Perda: 0.057513\n",
      "Época 875/1000, Perda: 0.057432\n",
      "Época 876/1000, Perda: 0.057352\n",
      "Época 877/1000, Perda: 0.057272\n",
      "Época 878/1000, Perda: 0.057191\n",
      "Época 879/1000, Perda: 0.057111\n",
      "Época 880/1000, Perda: 0.057031\n",
      "Época 881/1000, Perda: 0.056951\n",
      "Época 882/1000, Perda: 0.056871\n",
      "Época 883/1000, Perda: 0.056792\n",
      "Época 884/1000, Perda: 0.056712\n",
      "Época 885/1000, Perda: 0.056632\n",
      "Época 886/1000, Perda: 0.056553\n",
      "Época 887/1000, Perda: 0.056473\n",
      "Época 888/1000, Perda: 0.056394\n",
      "Época 889/1000, Perda: 0.056315\n",
      "Época 890/1000, Perda: 0.056236\n",
      "Época 891/1000, Perda: 0.056157\n",
      "Época 892/1000, Perda: 0.056078\n",
      "Época 893/1000, Perda: 0.055999\n",
      "Época 894/1000, Perda: 0.055920\n",
      "Época 895/1000, Perda: 0.055842\n",
      "Época 896/1000, Perda: 0.055763\n",
      "Época 897/1000, Perda: 0.055685\n",
      "Época 898/1000, Perda: 0.055607\n",
      "Época 899/1000, Perda: 0.055528\n",
      "Época 900/1000, Perda: 0.055450\n",
      "Época 901/1000, Perda: 0.055372\n",
      "Época 902/1000, Perda: 0.055294\n",
      "Época 903/1000, Perda: 0.055216\n",
      "Época 904/1000, Perda: 0.055139\n",
      "Época 905/1000, Perda: 0.055061\n",
      "Época 906/1000, Perda: 0.054984\n",
      "Época 907/1000, Perda: 0.054906\n",
      "Época 908/1000, Perda: 0.054829\n",
      "Época 909/1000, Perda: 0.054752\n",
      "Época 910/1000, Perda: 0.054675\n",
      "Época 911/1000, Perda: 0.054598\n",
      "Época 912/1000, Perda: 0.054521\n",
      "Época 913/1000, Perda: 0.054444\n",
      "Época 914/1000, Perda: 0.054368\n",
      "Época 915/1000, Perda: 0.054291\n",
      "Época 916/1000, Perda: 0.054215\n",
      "Época 917/1000, Perda: 0.054138\n",
      "Época 918/1000, Perda: 0.054062\n",
      "Época 919/1000, Perda: 0.053986\n",
      "Época 920/1000, Perda: 0.053910\n",
      "Época 921/1000, Perda: 0.053834\n",
      "Época 922/1000, Perda: 0.053758\n",
      "Época 923/1000, Perda: 0.053683\n",
      "Época 924/1000, Perda: 0.053607\n",
      "Época 925/1000, Perda: 0.053532\n",
      "Época 926/1000, Perda: 0.053457\n",
      "Época 927/1000, Perda: 0.053381\n",
      "Época 928/1000, Perda: 0.053306\n",
      "Época 929/1000, Perda: 0.053231\n",
      "Época 930/1000, Perda: 0.053156\n",
      "Época 931/1000, Perda: 0.053082\n",
      "Época 932/1000, Perda: 0.053007\n",
      "Época 933/1000, Perda: 0.052933\n",
      "Época 934/1000, Perda: 0.052858\n",
      "Época 935/1000, Perda: 0.052784\n",
      "Época 936/1000, Perda: 0.052710\n",
      "Época 937/1000, Perda: 0.052636\n",
      "Época 938/1000, Perda: 0.052562\n",
      "Época 939/1000, Perda: 0.052488\n",
      "Época 940/1000, Perda: 0.052414\n",
      "Época 941/1000, Perda: 0.052341\n",
      "Época 942/1000, Perda: 0.052267\n",
      "Época 943/1000, Perda: 0.052194\n",
      "Época 944/1000, Perda: 0.052121\n",
      "Época 945/1000, Perda: 0.052048\n",
      "Época 946/1000, Perda: 0.051975\n",
      "Época 947/1000, Perda: 0.051902\n",
      "Época 948/1000, Perda: 0.051829\n",
      "Época 949/1000, Perda: 0.051756\n",
      "Época 950/1000, Perda: 0.051684\n",
      "Época 951/1000, Perda: 0.051611\n",
      "Época 952/1000, Perda: 0.051539\n",
      "Época 953/1000, Perda: 0.051467\n",
      "Época 954/1000, Perda: 0.051395\n",
      "Época 955/1000, Perda: 0.051323\n",
      "Época 956/1000, Perda: 0.051251\n",
      "Época 957/1000, Perda: 0.051180\n",
      "Época 958/1000, Perda: 0.051108\n",
      "Época 959/1000, Perda: 0.051037\n",
      "Época 960/1000, Perda: 0.050965\n",
      "Época 961/1000, Perda: 0.050894\n",
      "Época 962/1000, Perda: 0.050823\n",
      "Época 963/1000, Perda: 0.050752\n",
      "Época 964/1000, Perda: 0.050681\n",
      "Época 965/1000, Perda: 0.050611\n",
      "Época 966/1000, Perda: 0.050540\n",
      "Época 967/1000, Perda: 0.050470\n",
      "Época 968/1000, Perda: 0.050399\n",
      "Época 969/1000, Perda: 0.050329\n",
      "Época 970/1000, Perda: 0.050259\n",
      "Época 971/1000, Perda: 0.050189\n",
      "Época 972/1000, Perda: 0.050119\n",
      "Época 973/1000, Perda: 0.050050\n",
      "Época 974/1000, Perda: 0.049980\n",
      "Época 975/1000, Perda: 0.049911\n",
      "Época 976/1000, Perda: 0.049841\n",
      "Época 977/1000, Perda: 0.049772\n",
      "Época 978/1000, Perda: 0.049703\n",
      "Época 979/1000, Perda: 0.049634\n",
      "Época 980/1000, Perda: 0.049565\n",
      "Época 981/1000, Perda: 0.049497\n",
      "Época 982/1000, Perda: 0.049428\n",
      "Época 983/1000, Perda: 0.049360\n",
      "Época 984/1000, Perda: 0.049291\n",
      "Época 985/1000, Perda: 0.049223\n",
      "Época 986/1000, Perda: 0.049155\n",
      "Época 987/1000, Perda: 0.049087\n",
      "Época 988/1000, Perda: 0.049019\n",
      "Época 989/1000, Perda: 0.048951\n",
      "Época 990/1000, Perda: 0.048884\n",
      "Época 991/1000, Perda: 0.048816\n",
      "Época 992/1000, Perda: 0.048749\n",
      "Época 993/1000, Perda: 0.048682\n",
      "Época 994/1000, Perda: 0.048615\n",
      "Época 995/1000, Perda: 0.048548\n",
      "Época 996/1000, Perda: 0.048481\n",
      "Época 997/1000, Perda: 0.048414\n",
      "Época 998/1000, Perda: 0.048348\n",
      "Época 999/1000, Perda: 0.048281\n",
      "Época 1000/1000, Perda: 0.048215\n",
      "Treinamento da rede concluído!\n"
     ]
    }
   ],
   "source": [
    "        # Configurar e treinar a MLP\n",
    "        num_entradas = 4  # Comprimento e largura da sépala e pétala\n",
    "        arquitetura = [8, 3]  # Camada oculta com 8 neurônios, saída com 3 neurônios\n",
    "        mlp = MLP(num_entradas, arquitetura, sigmoid, derivada_sigmoid)\n",
    "\n",
    "        taxa_aprendizado = 0.01  # Ajustado para convergência\n",
    "        num_epocas = 1000  # Aumentado para melhorar acurácia\n",
    "\n",
    "        print(\"\\nIniciando o treinamento da rede...\")\n",
    "        mlp.treinar(dados_treinamento_normalizados, rotulos_treinamento, taxa_aprendizado, num_epocas)\n",
    "        print(\"Treinamento da rede concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f895c3a",
   "metadata": {},
   "source": [
    "9. Avaliar a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ec0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acurácia da rede nos dados de teste: 100.00%\n"
     ]
    }
   ],
   "source": [
    "        # Avaliar no conjunto de teste\n",
    "        predicoes_teste = mlp.predizer(dados_teste_normalizados)\n",
    "        acuracia = np.mean(predicoes_teste == np.argmax(rotulos_teste, axis=1))\n",
    "        print(f\"\\nAcurácia da rede nos dados de teste: {acuracia * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d97f13",
   "metadata": {},
   "source": [
    "10. Exibir os Resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddce6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos da rede treinada:\n",
      "Camada 1:\n",
      "  Pesos:\n",
      "[[ 1.15016893 -0.21384892  2.00138745  1.3712416 ]\n",
      " [ 0.44955446 -0.51792804  1.38199105  3.02017465]\n",
      " [-0.36875648  1.43680448 -2.16904652 -1.50881715]\n",
      " [-1.27117077  2.06669216 -0.94206739 -4.02568084]\n",
      " [ 0.3334998   0.80765162 -1.10316648 -2.46956421]\n",
      " [ 0.760867    2.04814916  0.11022919  0.03278453]\n",
      " [ 0.04444544 -1.5178885   1.93301018  2.08561156]\n",
      " [-1.30561871  2.5464861  -2.64933719 -3.44387113]]\n",
      "  Bias:\n",
      "[-0.21307671 -2.92199949 -0.11596912  3.12985721  1.61114867 -0.48396974\n",
      " -0.38157642  0.28939568]\n",
      "Camada 2:\n",
      "  Pesos:\n",
      "[[-2.41527334e+00 -2.07360379e+00  2.18466032e+00  2.36936802e+00\n",
      "   9.12968221e-01  3.55087289e-03 -2.54890145e+00  2.59866888e+00]\n",
      " [ 9.45287678e-01 -2.56907773e+00 -1.37697006e+00  2.05142660e+00\n",
      "   9.87259857e-01 -2.16358107e+00  7.37961576e-01 -3.67103142e+00]\n",
      " [ 7.72308455e-01  3.46343233e+00 -1.32875298e+00 -4.62617049e+00\n",
      "  -2.93025224e+00 -2.19444977e-01  2.41818110e+00 -2.15593293e+00]]\n",
      "  Bias:\n",
      "[-0.97824549 -0.07014233 -0.48896015]\n",
      "\n",
      "Classificação das instâncias dos dados de teste:\n",
      "Instância 1: Características=[0.16       0.46666667 0.02173913 0.04347826], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 2: Características=[0.28       1.         0.06521739 0.        ], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 3: Características=[0.08       0.6        0.02173913 0.        ], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 4: Características=[0.36       0.93333333 0.04347826 0.        ], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 5: Características=[0.24       0.66666667 0.02173913 0.        ], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 6: Características=[0.24       0.8        0.         0.04347826], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 7: Características=[0.04       0.         0.         0.04347826], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 8: Características=[0.  0.6 0.  0. ], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 9: Características=[0.24       0.8        0.06521739 0.17391304], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 10: Características=[0.28       1.         0.13043478 0.08695652], Predito=Iris-setosa, Real=Iris-setosa\n",
      "Instância 11: Características=[0.52       0.46666667 0.63043478 0.43478261], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 12: Características=[0.52       0.4        0.63043478 0.47826087], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 13: Características=[0.72       0.4        0.65217391 0.47826087], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 14: Características=[0.28       0.13333333 0.36956522 0.39130435], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 15: Características=[0.52       0.33333333 0.60869565 0.47826087], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 16: Características=[0.44       0.2        0.67391304 0.43478261], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 17: Características=[0.68       0.46666667 0.7173913  0.52173913], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 18: Características=[0.56       0.2        0.58695652 0.43478261], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 19: Características=[0.24       0.         0.43478261 0.34782609], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 20: Características=[0.48       0.26666667 0.63043478 0.47826087], Predito=Iris-versicolor, Real=Iris-versicolor\n",
      "Instância 21: Características=[0.92       0.46666667 0.84782609 0.91304348], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 22: Características=[0.76       0.13333333 0.80434783 0.73913043], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 23: Características=[0.84       0.46666667 0.84782609 0.7826087 ], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 24: Características=[0.72       0.73333333 0.89130435 0.91304348], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 25: Características=[0.6        0.46666667 0.82608696 0.69565217], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 26: Características=[0.92       0.53333333 0.93478261 0.95652174], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 27: Características=[1.         0.53333333 0.82608696 0.91304348], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 28: Características=[0.56       0.26666667 0.82608696 0.73913043], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 29: Características=[0.96       0.6        1.         0.91304348], Predito=Iris-virginica, Real=Iris-virginica\n",
      "Instância 30: Características=[0.92       0.66666667 0.95652174 1.        ], Predito=Iris-virginica, Real=Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "        # Exibir pesos treinados\n",
    "        print(\"\\nPesos da rede treinada:\")\n",
    "        pesos_treinados = mlp.obter_pesos_treinados()\n",
    "        for nome_camada, pesos, bias in pesos_treinados:\n",
    "            print(f\"{nome_camada}:\")\n",
    "            print(\"  Pesos:\")\n",
    "            print(pesos)\n",
    "            print(\"  Bias:\")\n",
    "            print(bias)\n",
    "\n",
    "        # Exibir classificação das instâncias de teste\n",
    "        print(\"\\nClassificação das instâncias dos dados de teste:\")\n",
    "        for i, entrada in enumerate(dados_teste_normalizados):\n",
    "            predicao = predicoes_teste[i]\n",
    "            classe_predita = classes_teste[predicao]\n",
    "            classe_real = rotulos_teste_texto[i]\n",
    "            print(f\"Instância {i + 1}: Características={entrada}, Predito={classe_predita}, Real={classe_real}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
